{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import os.path as osp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3829, 3829, 3829, ..., 2236, 2236, 2228],\n",
       "       [3829, 3829, 3853, ..., 2236, 2236, 2228],\n",
       "       [3829, 3829, 3853, ..., 2236, 2236, 2228],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,  279,  279,  279],\n",
       "       [   0,    0,    0, ...,  279,  279,  279],\n",
       "       [   0,    0,    0, ...,  279,  279,  279]], dtype=int32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = '/media/arg_ws3/5E703E3A703E18EB/data/subt_real_ssd/JPEGImages/scene000001_4.png'\n",
    "img = Image.open(path)\n",
    "img = np.array(img)\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.3829, 0.3829, 0.3829, ..., 0.2236, 0.2236, 0.2228],\n",
       "       [0.3829, 0.3829, 0.3853, ..., 0.2236, 0.2236, 0.2228],\n",
       "       [0.3829, 0.3829, 0.3853, ..., 0.2236, 0.2236, 0.2228],\n",
       "       ...,\n",
       "       [0.    , 0.    , 0.    , ..., 0.0279, 0.0279, 0.0279],\n",
       "       [0.    , 0.    , 0.    , ..., 0.0279, 0.0279, 0.0279],\n",
       "       [0.    , 0.    , 0.    , ..., 0.0279, 0.0279, 0.0279]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = img/10000.\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "pil_img = Image.fromarray(img)\n",
    "transforms_ = [ transforms.Resize((256, 256), Image.BICUBIC),\n",
    "                transforms.ToTensor() ]\n",
    "transform_img = transforms.Compose(transforms_)\n",
    "pil_img = transform_img(pil_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3828, 0.3849, 0.3895,  ..., 0.2261, 0.2242, 0.2233],\n",
       "        [0.3850, 0.3878, 0.3916,  ..., 0.2261, 0.2242, 0.2233],\n",
       "        [0.3920, 0.3924, 0.3951,  ..., 0.2261, 0.2243, 0.2231],\n",
       "        ...,\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0180, 0.0142, 0.0193],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0299, 0.0286, 0.0290],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0280, 0.0279, 0.0279]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pil_img = pil_img.cpu().squeeze(0).detach()\n",
    "pil_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-bf996e1a9194>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpil_img\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "pil_img.mul(10000).clamp(0, 10000).to(torch.int16).permute(1, 2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[3828, 3849, 3894, ..., 2261, 2242, 2232],\n",
       "        [3849, 3877, 3916, ..., 2261, 2242, 2232],\n",
       "        [3920, 3923, 3951, ..., 2261, 2242, 2230],\n",
       "        ...,\n",
       "        [   0,    0,    0, ...,  179,  141,  192],\n",
       "        [   0,    0,    0, ...,  299,  286,  289],\n",
       "        [   0,    0,    0, ...,  280,  279,  278]]], dtype=uint16)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_img = (np.array(pil_img)*10000).astype(np.uint16)\n",
    "cv_img"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
